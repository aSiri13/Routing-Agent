{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aSiri13/Routing-Agent/blob/main/ReACT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Data + Build Query Engine Tools"
      ],
      "metadata": {
        "id": "kR5rJ5ZQbD6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Base ReAct Agent (gpt-3.5-turbo)\n",
        "\n",
        "This (not trained) ReAct agent can sometimes enter the incorrect reasoning loop to answer the question."
      ],
      "metadata": {
        "id": "tj0Z3hwpbORC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "id": "5Q5H46JoQEPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16fa387-3178-4a65-f043-04c0b7357e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.10.23-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.1.7-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.1.11-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.23 (from llama_index)\n",
            "  Downloading llama_index_core-0.10.23.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.1.12-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.1.12-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (3.9.3)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.23->llama_index) (4.10.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading PyMuPDF-1.24.0-cp310-none-manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index)\n",
            "  Downloading llama_parse-0.4.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.23->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.23->llama_index) (2.6.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.23->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.23->llama_index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.23->llama_index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.23->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.23->llama_index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.7.0)\n",
            "Collecting PyMuPDFb==1.24.0 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading PyMuPDFb-1.24.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.23->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.23->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.23->llama_index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.23->llama_index)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.23->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.23->llama_index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.23->llama_index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.23->llama_index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.23->llama_index) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.23->llama_index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, PyMuPDFb, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, pymupdf, httpcore, bs4, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed PyMuPDFb-1.24.0 bs4-0.0.2 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-agent-openai-0.1.7 llama-index-cli-0.1.11 llama-index-core-0.10.23.post1 llama-index-embeddings-openai-0.1.7 llama-index-indices-managed-llama-cloud-0.1.5 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.12 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.12 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.0 llama_index-0.10.23 llamaindex-py-client-0.1.13 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.14.3 pymupdf-1.24.0 pypdf-4.1.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent\n"
      ],
      "metadata": {
        "id": "3bvVuRcra6Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.tools.google import OpenCV\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "tool_spec = OpenCV()\n",
        "\n",
        "# Create a custom tool.\n",
        "def location() -> string:\n",
        "    return \"\"\n",
        "\n",
        "function_tool = FunctionTool.from_defaults(fn=location)\n",
        "\n",
        "tools = tool_spec.to_tool_list() + [function_tool]\n",
        "agent = OpenAIAgent.from_tools(tools, verbose=True)\n",
        "\n",
        "# use agent\n",
        "agent.chat(\n",
        "    \"Can you tell me where the apple is\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "6oBSebtUQhHh",
        "outputId": "24fc127d-f1c1-4e3f-c965-9e9bb7a9f161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index.tools'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5ed171e26122>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtool_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "setup llm and agent using base llm"
      ],
      "metadata": {
        "id": "AV7_iMHVbiAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
        "# llm = OpenAI(model=\"gpt-4-0613\")\n",
        "base_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "lTWpwPqWbdSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "7a05b97f-1bca-4c18-84ea-3e3122e4ecb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OpenAI' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-25de0f62fea7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-0613\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# llm = OpenAI(model=\"gpt-4-0613\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReActAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_engine_tools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OpenAI' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bad response below"
      ],
      "metadata": {
        "id": "EsyXLwTOcMpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = base_agent.chat(\n",
        "    \"User query goes here\"\n",
        ")\n",
        "\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2EmN8MbCbfsf",
        "outputId": "c438ebbf-6013-43d9-b6fb-e873163a4d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'base_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3b609aeac312>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = base_agent.chat(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"User query goes here\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base_agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bad response (x2)"
      ],
      "metadata": {
        "id": "Njlp-bXjcZn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = base_agent.chat(\n",
        "    \"User query goes here\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB2dAAYyboRK",
        "outputId": "111aff7c-446e-40ea-ec87-510e0f059dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: march_2022\n",
            "Action Input: {'input': 'risk factors'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: The company faces various risk factors that could have an adverse effect on its business, financial condition, and results of operations. These risk factors include the negative impact of economic, social, weather, and regulatory conditions on its operations, the potential failure or inferior performance of autonomous vehicle technologies, the need to retain and attract high-quality personnel, the risk of security breaches and data privacy breaches, the potential harm from cyberattacks, the impact of climate change risks, the reliance on third parties for distribution and software, the need for additional capital, the risks associated with identifying and integrating suitable businesses, the limitations on providing services in certain jurisdictions, the legal and regulatory risks, the risks related to payment and financial services regulations, the risks related to data processing and privacy, the risks related to intellectual property protection, and the volatility of the company's stock price.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools.\n",
            "Answer: In the quarter with the highest revenue growth, Uber faced various risk factors that could have an adverse effect on its business. These risk factors include the negative impact of economic, social, weather, and regulatory conditions on its operations, the potential failure or inferior performance of autonomous vehicle technologies, the need to retain and attract high-quality personnel, the risk of security breaches and data privacy breaches, the potential harm from cyberattacks, the impact of climate change risks, the reliance on third parties for distribution and software, the need for additional capital, the risks associated with identifying and integrating suitable businesses, the limitations on providing services in certain jurisdictions, the legal and regulatory risks, the risks related to payment and financial services regulations, the risks related to data processing and privacy, the risks related to intellectual property protection, and the volatility of the company's stock price. These risk factors highlight the challenges and uncertainties that Uber faces in its operations and growth.\n",
            "\u001b[0mIn the quarter with the highest revenue growth, Uber faced various risk factors that could have an adverse effect on its business. These risk factors include the negative impact of economic, social, weather, and regulatory conditions on its operations, the potential failure or inferior performance of autonomous vehicle technologies, the need to retain and attract high-quality personnel, the risk of security breaches and data privacy breaches, the potential harm from cyberattacks, the impact of climate change risks, the reliance on third parties for distribution and software, the need for additional capital, the risks associated with identifying and integrating suitable businesses, the limitations on providing services in certain jurisdictions, the legal and regulatory risks, the risks related to payment and financial services regulations, the risks related to data processing and privacy, the risks related to intellectual property protection, and the volatility of the company's stock price. These risk factors highlight the challenges and uncertainties that Uber faces in its operations and growth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Training/Eval Questions\n",
        "\n",
        "- Generate a synthetic dataset of questions to ask (later comes from visual Q&A dataset).\n",
        "- generate an initial set of questions over a “base” document (the March 2022 10Q)\n",
        "- use an LLM to generate variations of that question that can apply across multiple quarters (later apply across many types of sorroundings).\n",
        "\n",
        "Aim: stress-test the LLM reasoning capabilities."
      ],
      "metadata": {
        "id": "mxO5ImqiccmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llama_dataset.generator import RagDatasetGenerator"
      ],
      "metadata": {
        "id": "ffw5D_e9cTnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_question_gen_query = (\n",
        "    \"You are a Teacher/ Professor. Your task is to setup a quiz/examination.\"\n",
        "    \" Using the provided data to generate questions.\"\n",
        ")\n",
        "\n",
        "dataset_generator = RagDatasetGenerator.from_documents(\n",
        "    #march_docs, - replace with new data docs\n",
        "    question_gen_query=base_question_gen_query,\n",
        "    llm=llm_35\n",
        ")"
      ],
      "metadata": {
        "id": "QIhVGiu7cyD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to fix dataset generator - these questions will be from the CV & location Q&A dataset"
      ],
      "metadata": {
        "id": "kl0YtWSheumr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\"\"]"
      ],
      "metadata": {
        "id": "ltuGbxHrc1zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variations of each question"
      ],
      "metadata": {
        "id": "ayNPWfJEe7lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "\n",
        "vary_question_tmpl = \"\"\"\\\n",
        "You are a visual assistant. Given a question over a {data docs}, your goal\n",
        "is to generate up to {num_vary} variations of that question that might span multiple 10Q's.\n",
        "\n",
        "This can include compare/contrasting different 10Qs, replacing the current quarter with\n",
        "another quarter, or generating questions that can only be answered over multiple quarters (be creative!)\n",
        "\n",
        "You are given a valid set of 10Q filings. Please only generate question variations that can be\n",
        "answered in that set.\n",
        "\n",
        "For example:\n",
        "Base Question: What color is the apple?\n",
        "Valid 10Qs: [OpenCV, Knowledge Graph, Location]\n",
        "Question Variations:\n",
        "What color is the apple that is on the table?\n",
        "Can you compare/contrast the apple on the table to apples from grocery stores?\n",
        "\n",
        "Now let's give it a shot!\n",
        "\n",
        "Base Question: {base_question}\n",
        "Valid 10Qs: {valid_10qs}\n",
        "Question Variations:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gen_question_variations(base_questions, num_vary=3):\n",
        "    \"\"\"Generate question variations.\"\"\"\n",
        "\n",
        "    VALID_10Q_STR = \"[OpenCV, Knowledge Graph, Location]\"\n",
        "\n",
        "    llm = OpenAI(model=\"gpt-4\")\n",
        "    prompt_tmpl = PromptTemplate(vary_question_tmpl)\n",
        "\n",
        "    new_questions = []\n",
        "    for idx, question in enumerate(base_questions):\n",
        "        new_questions.append(question)\n",
        "        response = llm.complete(\n",
        "            prompt_tmpl.format(\n",
        "                num_vary=num_vary,\n",
        "                base_question=question,\n",
        "                valid_10qs=VALID_10Q_STR,\n",
        "            )\n",
        "        )\n",
        "        # parse into newlines\n",
        "        raw_lines = str(response).split(\"\\n\")\n",
        "        cur_new_questions = [l for l in raw_lines if l != \"\"]\n",
        "        print(f\"[{idx}] Original Question: {question}\")\n",
        "        print(f\"[{idx}] Generated Question Variations: {cur_new_questions}\")\n",
        "        new_questions.extend(cur_new_questions)\n",
        "\n",
        "    return new_questions\n",
        "\n",
        "\n",
        "def save_questions(questions, path):\n",
        "    with open(path, \"w\") as f:\n",
        "        for question in questions:\n",
        "            f.write(question + \"\\n\")\n",
        "\n",
        "\n",
        "def load_questions(path):\n",
        "    questions = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            questions.append(line.strip())\n",
        "    return questions"
      ],
      "metadata": {
        "id": "zNDZaPgbdNwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_questions = gen_question_variations(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "NrpY0RVRe4WQ",
        "outputId": "fce8eabd-de93-496a-8448-b6d2a4ace010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gen_question_variations' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8efe0d26ed47>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_question_variations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gen_question_variations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXquP21Pe-7r",
        "outputId": "860436a6-9313-4ee5-976c-b3e03ca7d532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "60 training questions, 20 evaluating questions"
      ],
      "metadata": {
        "id": "jjOblyAYrmrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions, eval_questions = new_questions[:60], new_questions[60:]"
      ],
      "metadata": {
        "id": "EZbN3N4Ef1LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_questions(train_questions, \"train_questions_10q.txt\")\n",
        "save_questions(eval_questions, \"eval_questions_10q.txt\")"
      ],
      "metadata": {
        "id": "RGrRG1r7f4fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions = load_questions(\"train_questions_10q.txt\")\n",
        "eval_questions = load_questions(\"eval_questions_10q.txt\")"
      ],
      "metadata": {
        "id": "2qL08w8Wf7sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Log Input/Output Pairs\n",
        "\n",
        "- run the train questions through a ReAct agent (GPT-4) to collect prompt outputs\n",
        "- Every prompt call to the LLM is logged as an input/output pair.\n",
        "- OpenAIFineTuningHandler automatically collects prompt input/outputs when agent queries are run.\n",
        "- This dataset can then be saved, in a dataset format .jsonl to directly feed to the OpenAI Finetuning endpoints to finetune 3.5 on 4 questions"
      ],
      "metadata": {
        "id": "GdsUJ9BxgNDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create finetuning_handler + callback_manager\n",
        "Also set context window length"
      ],
      "metadata": {
        "id": "70Nno06UggHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "finetuning_handler = OpenAIFineTuningHandler()\n",
        "callback_manager = CallbackManager([finetuning_handler])\n",
        "\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# limit the context window artifically to test refine process\n",
        "Settings.context_window = 2048"
      ],
      "metadata": {
        "id": "qdIvUxnzgLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup gpt-4 ReAct Agent"
      ],
      "metadata": {
        "id": "woT4YvwPgmkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-4-0613\")\n",
        "gpt4_agent = ReActAgent.from_tools(\n",
        "    query_engine_tools,\n",
        "    llm=llm,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "MM4NWVaxgfbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, question in enumerate(train_questions):\n",
        "    print(f\"[{idx}] Question: {question}\")\n",
        "    response = gpt4_agent.query(question)\n",
        "    print(f\"[{idx}] Agent Response: {str(response)}\")"
      ],
      "metadata": {
        "id": "B0naHIGWgo9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save events"
      ],
      "metadata": {
        "id": "iB_jLUp_j8h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_handler.save_finetuning_events(\"finetuning_events_10q.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt2T-mDdgub1",
        "outputId": "7ab01dc1-3967-431a-ba14-205c3704abcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 179 examples to finetuning_events_10q.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create OpenAIFinetuneEngine\n",
        "the finetune engine will launch a finetuning job, and returning an LLM model that you can directly plugin to the rest of LlamaIndex workflows."
      ],
      "metadata": {
        "id": "Iv-zWwmWkEX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If finetuning the same model even further - list job id below"
      ],
      "metadata": {
        "id": "I8PHKTiPrKfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.finetuning import OpenAIFinetuneEngine\n",
        "\n",
        "finetune_engine = OpenAIFinetuneEngine(\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"finetuning_events_10q.jsonl\",\n",
        "    #start_job_id=\"<start-job-id>\"  # if you have an existing job, can specify id here\n",
        ")"
      ],
      "metadata": {
        "id": "ey5MNZUWj9z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finetune"
      ],
      "metadata": {
        "id": "o493gxOukxpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_engine.finetune()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOBteXUKkLSa",
        "outputId": "bcb9fc76-9790-4834-835a-8a4a4a7423df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 179\n",
            "First example:\n",
            "{'role': 'system', 'content': '\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: march_2022\\nTool Description: Provides information about Uber quarterly financials ending March 2022\\nTool Args: {\"type\": \"object\", \"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"]}\\n\\n> Tool Name: june_2022\\nTool Description: Provides information about Uber quarterly financials ending June 2022\\nTool Args: {\"type\": \"object\", \"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"]}\\n\\n> Tool Name: sept_2022\\nTool Description: Provides information about Uber quarterly financials ending September 2022\\nTool Args: {\"type\": \"object\", \"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"]}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of march_2022, june_2022, sept_2022) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n'}\n",
            "{'role': 'user', 'content': \"What is the address of Uber Technologies, Inc.'s principal executive offices?\"}\n",
            "{'role': 'assistant', 'content': \"Thought: The question is about the address of Uber's principal executive offices. This information is not related to Uber's financials, so the tools I have access to (march_2022, june_2022, sept_2022) are not relevant for this task.\\n\\nAnswer: Sorry, I cannot answer your query.\"}\n",
            "No errors found\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 11\n",
            "mean / median: 5.346368715083799, 5.0\n",
            "p5 / p95: 3.0, 9.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 607, 2162\n",
            "mean / median: 835.1229050279329, 751.0\n",
            "p5 / p95: 626.8, 1147.8000000000006\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 38, 684\n",
            "mean / median: 140.3240223463687, 106.0\n",
            "p5 / p95: 49.8, 273.60000000000014\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
            "Dataset has ~149487 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~448461 tokens\n",
            "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n",
            "This means your total cost for training will be $1.1958959999999998 per epoch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get finetuning job id"
      ],
      "metadata": {
        "id": "7FojFuTMlMoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune_engine.get_current_job()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpoefPFlkyZ8",
        "outputId": "0dfb1dc8-8110-468a-afc5-c594800c5f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-qiGoHWTJZfc6iun19unXc2pV', created_at=1709675066, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-by7b0ByCCb8VyGQAQud5kEuU', result_files=[], status='queued', trained_tokens=None, training_file='file-Ii4qXOwTGc4VMrScvv0iZacL', validation_file=None, user_provided_suffix=None)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get finetuned llm - see status on OAI dev portal (must be successful)"
      ],
      "metadata": {
        "id": "jLBUVQwelWT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_llm = finetune_engine.get_finetuned_model(temperature=0.3)"
      ],
      "metadata": {
        "id": "97qfMQmQlOdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare Finetuned Agent vs. Base Agent\n",
        "- run some sample queries from the evaluation dataset over both our finetuned agent as well as the base agent.\n",
        "- qualitatively look at their abilities to perform chain of thought prompting in order to arrive at the right answer."
      ],
      "metadata": {
        "id": "5VD0AT7fsVYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup FT agent using ft model"
      ],
      "metadata": {
        "id": "vn7KEBOdshdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_agent = ReActAgent.from_tools(\n",
        "    query_engine_tools,\n",
        "    llm=ft_llm,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "B78B66jplX-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieve evaluation questions"
      ],
      "metadata": {
        "id": "3NSHwC1lsowy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\"Is the teacher standing in front of the classroom while giving a lecture or conducting an activity?\",\n",
        "  \"How does the classroom arrangement affect the teacher's ability to interact with students at their desks?\",\n",
        "  \"Is the student writing an answer on the whiteboard as part of a group activity or individual response?\",\n",
        "  \"Are the books on the teacher's desk related to the subject currently being taught or for future lessons?\",\n",
        "  \"Is the projector used more frequently for presentations or for showing educational videos?\",\n",
        "  \"How do the dynamics of students working in groups compare to individual work in terms of engagement and productivity?\",\n",
        "  \"Can the student raising their hand to ask a question indicate a high level of classroom participation?\",\n",
        "  \"Is the teacher using the map on the wall for a geography lesson or historical context?\",\n",
        "  \"Do open windows in the classroom contribute to a better learning environment by improving air quality?\",\n",
        "  \"Is the student using their laptop for research, writing, or browsing unrelated content?\",\n",
        "  \"Are the pencils and pens organized by color or type in the holder?\",\n",
        "  \"Is the teacher reading from a textbook as the primary source of information or supplementing with external materials?\",\n",
        "  \"Are students more focused when watching an educational video compared to a traditional lecture?\",\n",
        "  \"Is the clock in the classroom an effective tool for time management for both students and the teacher?\",\n",
        "  \"How does the stress level of students taking a test compare with their usual classroom work?\",\n",
        "  \"Is the teacher checking homework assignments for completeness or understanding of the material?\",\n",
        "  \"Are the students' drawings in their sketchbooks related to the current curriculum or personal projects?\",\n",
        "  \"Is the student using a tablet for educational apps, reading, or entertainment?\",\n",
        "  \"How effective are audio lessons through headphones in maintaining students' concentration compared to classroom discussions?\",\n",
        "  \"Is the teacher explaining a complex math problem on the projector screen more effective than traditional board work?\",\n",
        "  \"Is the public library serving more as a study space or a resource center for the community?\",\n",
        "  \"Can the proximity to a coffee shop enhance the appeal of a location for students or professionals?\",\n",
        "  \"Does the presence of a lake in the nearest park increase its popularity for recreational activities?\",\n",
        "  \"How do Italian restaurants in the neighborhood compare in terms of authenticity and price?\",\n",
        "  \"Is the accuracy of the weather forecast impacting planning for outdoor activities in the area?\",\n",
        "  \"Which is a bigger tourist attraction, the museum or the cinema, based on their proximity?\",\n",
        "  \"Do grocery stores offering organic produce attract a more health-conscious clientele?\",\n",
        "  \"How does the presence of a pet-friendly café within a 1-mile radius affect the popularity of the area?\",\n",
        "  \"Are public transportation options adequate for commuters to the city center during peak hours?\",\n",
        "  \"Does the region's recognition for historical landmarks contribute to its cultural significance?\",\n",
        "  \"How reliable are gym opening hours for early morning or late-night fitness enthusiasts?\",\n",
        "  \"Are flu vaccinations at the closest pharmacy drawing a larger crowd during flu season?\",\n",
        "  \"Does being designated as a nature reserve increase the ecological awareness among visitors?\",\n",
        "  \"How does the availability of a nearby bicycle rental service encourage eco-friendly tourism?\",\n",
        "  \"Are lifeguards at the nearest beach adequately trained for emergency situations?\",\n",
        "  \"How does the high population density of a zone affect the quality of life for its residents?\",\n",
        "  \"Is the daily air quality index a significant concern for people with respiratory issues in the area?\",\n",
        "  \"How do vegan restaurants within a 10-minute drive cater to the diverse dietary preferences of the community?\",\n",
        "  \"Is the direct train service to the airport reliable for timely travel?\",\n",
        "  \"Does the presence of a swimming pool at the nearest hotel enhance its appeal to tourists?\",\n",
        "  \"How does the teacher's posture change when standing in front of different sections of the classroom?\",\n",
        "  \"Can the students at their desks be grouped by their level of attentiveness?\",\n",
        "  \"Which student is the most active in writing answers on the whiteboard, and how does their activity compare to the rest?\",\n",
        "  \"Are the books on the teacher's desk more related to the current subject being taught or for future lessons?\",\n",
        "  \"What is the topic of the presentation being displayed by the projector, and how does it relate to the students' current curriculum?\",\n",
        "  \"How do the dynamics of student groups vary across different classroom activities?\",\n",
        "  \"What is the frequency of students raising their hands to ask questions during a typical class session?\",\n",
        "  \"Can the teacher's pointing to the map on the wall be correlated with the geographic focus of the current lesson?\",\n",
        "  \"What is the impact of open windows in the classroom on the students' attentiveness and classroom temperature?\",\n",
        "  \"Is the student typing on a laptop more engaged in academic activities or non-educational browsing?\",\n",
        "  \"How does the organization of pencils and pens in the holder reflect the overall tidiness of the classroom?\",\n",
        "  \"What subject is the teacher reading from the textbook, and how does it align with the students' interest levels?\",\n",
        "  \"Are students more engaged when watching an educational video or during interactive classroom activities?\",\n",
        "  \"How does the classroom's atmosphere change when the clock indicates it's time for recess?\",\n",
        "  \"What strategies do students use to concentrate while taking a test in a noisy classroom?\",\n",
        "  \"In what ways does the teacher provide feedback while checking homework assignments, and how is it received by the students?\",\n",
        "  \"How do the themes of students' drawings in their sketchbooks relate to the subjects being taught in class?\",\n",
        "  \"What content is the student primarily accessing on the tablet, educational or entertainment?\",\n",
        "  \"How does listening to an audio lesson through headphones affect students' comprehension compared to traditional teaching methods?\",\n",
        "  \"What are the most common errors made by students when solving the math problem explained on the projector screen?\",\n",
        "  \"What distinct characteristics set a public library apart from a school library in terms of ambiance and resources?\",\n",
        "  \"What are the most popular destinations within walking distance of the nearest coffee shop, and why?\",\n",
        "  \"Does the presence of a lake in the nearest park increase its popularity among local residents?\",\n",
        "  \"How do Italian restaurants in the current neighborhood compare in terms of authenticity and price?\",\n",
        "  \"What are the implications of today's weather forecast on outdoor activities in the area?\",\n",
        "  \"Which is a more popular local attraction, the museum or the cinema, and what factors contribute to its popularity?\",\n",
        "  \"What variety of organic produce is offered by the closest grocery store, and how does it compare to non-organic options?\",\n",
        "  \"What amenities do pet-friendly cafés within a 1-mile radius offer to attract pet owners?\",\n",
        "  \"How reliable and timely are the public transportation services from this location to the city center?\",\n",
        "  \"What historical landmarks in the region are most visited, and what stories do they tell?\",\n",
        "  \"What are the peak hours for the nearest gym, and how crowded does it get?\",\n",
        "  \"Are there any other health services provided by the closest pharmacy besides flu vaccinations?\",\n",
        "  \"What flora and fauna can be found in the area designated as a nature reserve?\",\n",
        "  \"How does the availability of bicycle rental services impact the local culture and transportation habits?\",\n",
        "  \"What safety measures are in place at the nearest beach, and how do they affect visitors' experiences?\",\n",
        "  \"How does the high population density in the zone impact the local community and infrastructure?\",\n",
        "  \"What actions are being taken in the area to improve the air quality index to a healthy level?\",\n",
        "  \"What unique offerings do vegan restaurants within a 10-minute drive provide to attract customers?\",\n",
        "  \"What are the advantages and disadvantages of using the direct train service from the current location to the airport?\",\n",
        "  \"What additional amenities does the nearest hotel offer to enhance guests' stays beyond having a swimming pool?\"]\n",
        "with open(\"eval_questions_10q.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        eval_questions.append(line.strip())"
      ],
      "metadata": {
        "id": "B7oTX9qJsnUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qidx = 0\n",
        "print(eval_questions[qidx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB2RJjJLsqz0",
        "outputId": "226f0aa6-d5df-4a9e-9ec6-ed2641065571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the total fair value of Uber's financial assets as of March 31, 2022?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get response from base agent\n",
        "base agent = normal gpt-3.5"
      ],
      "metadata": {
        "id": "kKKZBS8Ns2G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_response = base_agent.query(eval_questions[qidx])\n",
        "print(str(base_response))"
      ],
      "metadata": {
        "id": "aJvdhaTXssGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get response from ft agent\n",
        "ft agent = finetuned gpt 3.5 on answers from gpt 4.0"
      ],
      "metadata": {
        "id": "56wJTB76tFL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_response = ft_agent.query(eval_questions[qidx])\n",
        "print(str(ft_response))"
      ],
      "metadata": {
        "id": "CplWxuZks5is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TL;DR\n",
        "- finetuned model does much better than the base model in terms of reasoning about the current sequence of steps @same costs.\n",
        "- It passes more detailed answers to the downstream tools\n",
        "- Is more capable of refining its approach when initial queries don’t work."
      ],
      "metadata": {
        "id": "vtPdDJ0Rt65h"
      }
    }
  ]
}